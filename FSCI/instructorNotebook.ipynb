{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying How Scientific Papers Are Shared and Who Is Sharing Them on Twitter\n",
    "\n",
    "Welcome to the data collection and analysis part of [_Identifying How Scientific Papers Are Shared and Who Is Sharing Them on Twitter_](https://docs.google.com/document/d/1mKA4p5m7ubqJuyTDbDOm9J0F4wYBUoW9t1zhmWEN__M/edit#heading=h.esjw8tv92vde). In this part of the course, you will see, be able to execute, and modify the code that is necessary to collect user information from Twitter, including all of the follower relationships, using Python. This data will then be used to construct networks, and to calculate network statistics for analysis. \n",
    "\n",
    "This webpage you are looking at is a [Jupyter Notebook](http://jupyter.org/). It allows for a combination of HTML (like what you are reading now) and code (like you see immediately below). The code can be executed by clicking on the cell (the text area with the code in it) and then hitting `Ctrl + Enter`. \n",
    "\n",
    "Try it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) intro cell\n",
    "\n",
    "byt = b')\\x16\\x01H\\x05\\x1b\\x03\\rT\\r\\x17\\x0b\\x13\\x0c\\x00\\r\\x0bN\\x03\\x16\\x19\\rO\\x1e\\t\\r\\x1c\\x07\\x01N\\x13\\x16\\x10\\rN'\n",
    "bytes(c ^ b'python'[i % 6] for i, c in enumerate(byt)).decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an \"Out\" label on the left, and then some words. \n",
    "\n",
    "Once you did that, we are ready to get started. The first step, however, is to get ourselves programatic access to Twitter. For this, you will need to have a Twitter account. It can be your personal Twitter account, since we are only planning on reading information from Twitter, and not posting any actual tweets (unless you want to). Once you're logged in to your Twitter account, go to [https://apps.twitter.com/](https://apps.twitter.com/). \n",
    "\n",
    "Once there, click on \"Create New App\", and fill in the form. The `name`, `description`, and `website` you provide do not matter. You can leave the `callback URL` blank. Agree to the terms of service and create the app. \n",
    "\n",
    "One the second page, choose the `Keys and Access Tokens` tab and then, near the bottom, `Create Access Tokens`. \n",
    "\n",
    "Now you have everything you need to programatically interact with Twitter. The Consumer Key and Secret (near the top of the page) and the Access Token and Secret (on the bottom) essentially replace your username and password. \n",
    "\n",
    "Enter the four values below. Make sure you place the values between single quotes. For example: \n",
    "\n",
    "`consumer_key = 'youareawesomekeepitup'`\n",
    "\n",
    "**run the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) setup cell \n",
    "\n",
    "# These are some Python modules we may need, so we are loading them here\n",
    "import tweepy\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, time, os, sys\n",
    "\n",
    "# Add in your four values below. The access usually has a hyphen in it\n",
    "# so make sure you get it all when you copy paste\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note: This server is hosted by Digital Ocean and Juan Pablo Alperin is the only person that currently has access. It is essentially secure, but you should be aware that when you save your Jupyter notebook, you are saving those Twitter credentials on the server and everyone in the course could see your keys and tokens. There is little risk within this closed setting, but you should probably revoke the tokens after the course is done and make new ones. \n",
    "\n",
    "----\n",
    "\n",
    "So far you just saved those keys into some variables, but we have not done anything with the Twitter API. The [Twitter API](https://dev.twitter.com/rest/public) is a defined set of commands you can give Twitter to read and write Twitter data. It is essentially a programmatic way of doing all the things you can usually do on Twitter: read tweets, see who follows who, look at timelines, etc. \n",
    "\n",
    "Fortunately, we do not need to worry about writing all the Python code to do all the nitty gritty. There is a Python module called [Tweepy](http://pythoncentral.io/introduction-to-tweepy-twitter-for-python/) that does most of it for us. \n",
    "\n",
    "**Run the cell below.** *In fact, every time you see a code cell, run it before proceeding. Subsequent code might rely on it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) authentication cell \n",
    "\n",
    "# Ask Tweepy to authenticate you to Twitter (log you in)\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Set up access to the Twitter API using the authentication info and some options\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "print('You are now logged in as: %s (%s)' % (api.me().screen_name, api.me().name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you are logged in to Twitter through the API using Tweepy! We are ready to really get going. \n",
    "\n",
    "Let's try a few things so that you can get a sense of how the API works. You can see a full list of the methods Tweepy makes available by looking at [their documentation](http://docs.tweepy.org/en/v3.5.0/api.html). \n",
    "\n",
    "We are going to start with the **[get_status](http://docs.tweepy.org/en/v3.5.0/api.html#API.get_status)** method. \n",
    "\n",
    "**Run the cell below. Then try changing the Tweet ID (last part of every tweet URL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) fetch tweet cell\n",
    "\n",
    "# Twitter calls tweet 'statuses'\n",
    "status = api.get_status(870093876853227520)\n",
    "\n",
    "# Print the time the tweet was created\n",
    "print(\"Created at: %s\" % status.created_at)\n",
    "\n",
    "# Print the text of the tweet\n",
    "print(\"Text: %s\" % status.text)\n",
    "\n",
    "# Print the author of the tweet\n",
    "print(\"Author: %s\" % status.author.screen_name)\n",
    "\n",
    "# Loop through the hashtags of the tweet and print them (if there are any)\n",
    "if 'hashtags' in status.entities: \n",
    "    print(\"hashtags: %s\" % ', '.join([hashtag['text'] for hashtag in status.entities['hashtags']]))\n",
    "        \n",
    "# Loop through the urls of the tweet and print them (if there are any)\n",
    "if 'urls' in status.entities: \n",
    "    print(\"urls: %s\" % ', '.join([url['expanded_url'] for url in status.entities['urls']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the API to fetch information about specific users using the **[get_user](http://docs.tweepy.org/en/v3.5.0/api.html#API.get_user)** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) fetch user cell\n",
    "\n",
    "# We can use the API object to see who we are logged in as\n",
    "# You can also change your_screen_name to any other screen name (remember to put it in 'single_quotes')\n",
    "your_screen_name = api.me().screen_name\n",
    "\n",
    "# Now we can use Tweepy to get information about your user\n",
    "user = api.get_user(your_screen_name)\n",
    "\n",
    "print(user.location)\n",
    "\n",
    "print(\"You first joined Twitter on: %s\" % user.created_at)\n",
    "print(\"This is what your bio says: \")\n",
    "print(user.description)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also get information about who follows you with the **[followers](http://docs.tweepy.org/en/v3.5.0/api.html#API.followers)** method, and about who you follow using the **friends** method (for some reason, not in Twitter documentation). \n",
    "\n",
    "There are also two other methods for accessing followers and friends, that allow you to get many more users at a time. These are the **[followers_ids](http://docs.tweepy.org/en/v3.5.0/api.html#API.followers_ids)** and **[friends_ids](http://docs.tweepy.org/en/v3.5.0/api.html#API.friends_ids)**. These latter two methods return a list of user_ids, while the first two return a list of user objects. \n",
    "\n",
    "That is, the first two give you all the information about the actual followers/friends (screen names, descriptions, etc.), while the latter two give you a list of the ids only. While the first two are more detailed, they only return 100 people at a time, while the latter two return 1,000 people at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) fetch followers cell \n",
    "\n",
    "# Get a list of the last 100 of the people who follow you\n",
    "followers = api.followers(user.id)\n",
    "# print out the most recent one. The [0] says the first item on the list\n",
    "print(\"The last person who followed you was: @%s\" % followers[0].screen_name)\n",
    "\n",
    "# Get a list of the last 100 people you followed\n",
    "friends = api.friends(user.id)\n",
    "print(\"The last person you followed was: @%s\" % friends[0].screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the URL's of a tweet from each of you. Imagine these were tweets that all had the URL to the same academic article, which is what we essentially get from Altmetric. \n",
    "\n",
    "You can change, add, or remove tweets from this list, and the rest of the code will work. \n",
    "\n",
    "**Note: You probably don't want to add too many users, because it will slow down the data collection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g) setup our sample cell \n",
    "\n",
    "sample = [\n",
    "'https://twitter.com/juancommander/status/870093876853227520',\n",
    "'https://twitter.com/mauvepg/status/909456056748617728',\n",
    "'https://twitter.com/ScottSteedman2/status/1094281373999755264',\n",
    "'https://twitter.com/learnpublishing/status/1108952174237630464', \n",
    "    \n",
    "'https://twitter.com/MooreaCorrigan/status/1098472426369765376',\n",
    "'https://twitter.com/loramouammer_/status/1024831021408116736', \n",
    "'https://twitter.com/astrpt/status/1112149646934044672', \n",
    "'https://twitter.com/wholeheartedeat/status/1102639436649648128',\n",
    "'https://twitter.com/JazminWelch/status/1111150156982812672'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your original source of data may be very different, but in the end, what you want is a list of tweet_ids. If you had a CSV or an Excel file with a list of IDs, you'd just need to write a few lines of additional Python code to read those into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) extract tweet id from sample cell\n",
    "\n",
    "# We are just going to extract the tweet ID (the numeric part at the end of the URL)\n",
    "tweet_ids = [] # start an empty list\n",
    "for tweet_url in sample:\n",
    "    tweet_id = tweet_url.split('/')[-1] # split on the /, then grab the last part [-1]\n",
    "    tweet_ids.append(tweet_id) # add tweet ID to our list\n",
    "\n",
    "# This should now have a list of all the tweet_ids\n",
    "print(tweet_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll loop through that list of tweet IDs to fetch all of the tweet details. Tweepy will put each tweet into a `Status Object`. The code below fetches from the API, using the same `get_status` method we used above, and appends it to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) fetch many tweets cell\n",
    "\n",
    "statuses = [] # start a list of empty statuses\n",
    "for tweet_id in tweet_ids:\n",
    "    # The try/except block is just in case there is an error\n",
    "    # instead of stopping on the error, it will spit it out and keep going\n",
    "    # None of the above tweets should cause errors\n",
    "    try: \n",
    "        statuses.append(api.get_status(tweet_id))\n",
    "    except tweepy.TweepError as error: \n",
    "        print('Had a problem getting tweet_id = %s' % tweet_id)\n",
    "        print(error)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above could also have been done with a single line (and a single API call), using the [statuses_lookup](http://docs.tweepy.org/en/v3.5.0/api.html#API.statuses_lookup) endopint, which allows querying up to 100 tweets at a time: \n",
    "\n",
    "`statuses = api.statuses_lookup(tweet_ids)` \n",
    "\n",
    "In those statuses, we get all of the user objects. So we can just loop through and find out who the users are behind those tweets. We already had the screen_names in the URLs, but this will allow us to have the user_ids (the numeric identifier) for each user. While users can change their screen names over time, their numeric identifier stays the same, so fetching the tweet_id, and using that to get the user_id, is the most robust way of ensuring you have the right user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j) get user from tweet cell \n",
    "\n",
    "users = {}  # Make an empty dictionary. It will have a mapping of user_id => User Object\n",
    "# dictionaries let us have a collection of objects, like a list, but with an index so we can later\n",
    "# retrieve any individual item. \n",
    "# We can then do something like users[12345] and it will return the User object for user with id 12345\n",
    "\n",
    "# Loop through each of the statuses, and identify the tweet's author\n",
    "for status in statuses: \n",
    "    user_id = status.author.id\n",
    "    users[user_id] = status.author  # Make a mapping of user_id => User Object\n",
    "\n",
    "print(\"These are all the user ids we have collected:\")\n",
    "print(users.keys())\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print('Here is what the first user object looks like:')\n",
    "first_user_id = list(users.keys())[0]\n",
    "print(json.dumps(users[first_user_id]._json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the users dictionary has a mapping of user_id => user object for all users in our sample of tweets. For our network project, we are really only going to use the user_ids, but you can imagine scenarios where you want to use the rest of the user object. For example, to extract information from the descriptions, their location, etc. \n",
    "\n",
    "For example, let's see who is the user that has tweeted the most. \n",
    "\n",
    "Or let's see how many users have set their location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k) analyze users cell\n",
    "\n",
    "# Set the maximum to 0 to start, and the user to empty\n",
    "max_statuses = 0\n",
    "max_statuses_user = None\n",
    "\n",
    "for user_id, user in users.items():\n",
    "    # If the next user in the loop has more than the max we found\n",
    "    # save that new maximum and the user that goes along with it\n",
    "    if user.statuses_count > max_statuses:\n",
    "        max_statuses = user.statuses_count\n",
    "        max_statuses_user = user  \n",
    "        \n",
    "print('%s has the most tweets at: %s' % (max_statuses_user.screen_name, max_statuses))\n",
    "\n",
    "has_location = 0\n",
    "has_geo_enabled = 0\n",
    "for user_id, user in users.items():\n",
    "    if len(user.location) > 0:\n",
    "        has_location += 1\n",
    "    if user.geo_enabled:\n",
    "        has_geo_enabled +=1\n",
    "print('%i people (%.2f%%) have something in their location field.' % (has_location, 100.0*has_location/len(users)))\n",
    "print('%i people (%.2f%%) have geolocation enabled.' % (has_geo_enabled, 100.0*has_geo_enabled/len(users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get to constructing the network! For this we need to get all of the followers for every person in our sample. We will use the [followers_ids](http://docs.tweepy.org/en/v3.5.0/api.html#API.followers_ids) endpoint, which lets us fetch up to 5,000 followers at a time. That means we need to do two API calls for someone with 8,000 followers, three for someone with 11,000, etc. \n",
    "\n",
    "The API allows us to do 15 calls to this endpoint every 15 minutes, so if someone has a lot of followers, or if you have too many people, there might be a need for a 15 minute break while this finishes.\n",
    "\n",
    "This use of the API is a little different, because we need to \"page\" through the results if a user has more than 5,000 followers. The followers_ids endpoint will return 5,000 on each page, and we need to move through each page until we get them all. Fortunately, Tweepy handles all of this for us with something called a Cursor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l) get all followers cell\n",
    "\n",
    "followers = {} # Make an empty dict, where we'll put user_id = [list of follower ids]\n",
    "for user_id, user in users.items():\n",
    "    ids = []\n",
    "    \n",
    "    # The Cursor method of Tweepy is for things that can take multiple pages to complete\n",
    "    # Combined with the .pages() at the end, they return something we can loop through\n",
    "    # So then we add a 'for page', which will then loop us through all the pages of results\n",
    "    for page in tweepy.Cursor(api.followers_ids, id=user_id).pages():\n",
    "        ids.extend(page)  # Append the list that came back with the list of each page\n",
    "        print('Adding %s followers for %s' % (len(page), user.screen_name))\n",
    "\n",
    "    followers[user_id] = ids # Save all of these followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to write the edgelist (the follower/friend information) so that we can then construct the network. \n",
    "\n",
    "**Be sure to set your participant number below, or you will not be able to save files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m) set your working directory cell\n",
    "\n",
    "# Put your number here. This will avoid you overwriting each other's work.\n",
    "student_number = 'instructor'\n",
    "\n",
    "datadir = 'data/%s' % student_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n) write out the edge list cell\n",
    "\n",
    "with open('%s/participant_edgelist.csv' % datadir, 'w') as f:    \n",
    "    # Loop through our dict of follower lists\n",
    "    for user_id, list_of_followers in followers.items():\n",
    "        # For each user, loop through each of their followers\n",
    "        for follower in list_of_followers:\n",
    "            # Write an edge from our user (one of you) to each of your followers\n",
    "            # Direction indicates information can go from you to them\n",
    "            f.write('%s\\t%s\\n' % (user_id, follower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `participant_edgelist.csv` now has all of our network information. You can see a random sample of it with the following command, which is using the [Python Pandas](http://pandas.pydata.org/) module. Pandas is a data analysis library which is commonly used for data science work, but is beyond the scope of this workshop. \n",
    "\n",
    "You can run the command multiple times for different samples. You'll notice that these are using the numeric user_ids, not screen_names, for the reasons outlined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o) look inside the edge list file\n",
    "\n",
    "pd.read_csv('%s/participant_edgelist.csv' % datadir, sep='\\t', header=None, names=['Source', 'Target']).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an edgelist, we can start using a network analysis library to do some network specific stuff. For that, we'll use the [Python iGraph](http://igraph.org/python/) module. There are several other network analysis toolkits for Python, namely [NetworkX](https://networkx.github.io/) and Stanford's [SNAP for Python](https://snap.stanford.edu/snappy/). All have their pros and cons. \n",
    "\n",
    "In my opinion, iGraph strikes a balance between being complete, fast, and intuitive. However, opinions on this may vary wildly. iGraph does have the downside of having pretty terrible documentation. Even the [tutorial](http://igraph.org/python/doc/tutorial/tutorial.html) is pretty incomplete. \n",
    "\n",
    "All of the following tasks could be completed with any of the above listed softwares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p) import iGraph cell\n",
    "try: \n",
    "    reload(igraph)\n",
    "except:\n",
    "    import igraph as ig # When installing this library, be sure to install python-igraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [Read_Ncol](http://igraph.org/python/doc/igraph.GraphBase-class.html#Read_Ncol) to read in an edgelist, where we consider the two columns to be the \"names\" of the nodes. Because our edgelist is made up of user_ids, these will still be a bunch of numbers, but it will make it easier for us to keep track of who is who."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q) read in your edgelist into iGraph cell\n",
    "\n",
    "G = ig.Graph.Read_Ncol('%s/participant_edgelist.csv' % datadir, names=True, directed=True)\n",
    "print(\"The graph of everyone now has %s nodes and %s edges\" % (G.vcount(), G.ecount()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a subgraph that only leaves all of you, and removes your followers. Note that this will keep all the edges between you, so we have a subgraph, which tells us who follows who in this group.\n",
    "\n",
    "We'll use the [subgraph](http://igraph.org/python/doc/igraph.Graph-class.html#subgraph) method, that expects us to pass in the list of user_ids we want to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r) make a subgraph cell\n",
    "\n",
    "# We need to convert the numerical IDs to strings, because it is what iGraph expects\n",
    "user_ids = [str(user_id) for user_id in users.keys()]\n",
    "\n",
    "subG = G.subgraph(user_ids)\n",
    "print(\"This subgraph of just you now has %s nodes and %s edges\" % (subG.vcount(), subG.ecount()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s) add attributes to nodes and look at them cell\n",
    "\n",
    "# subG.vs has the list of vertices \n",
    "# we want to add the screen_name as an attribute\n",
    "# We can find those in the users dictionary from before\n",
    "for v_index in range(subG.vcount()):\n",
    "    user_id = int(subG.vs[v_index]['name'])  # the 'name' field has the user_id, convert to int\n",
    "    subG.vs[v_index]['screen_name'] = users[user_id].screen_name\n",
    "    \n",
    "# Let's check the first few\n",
    "[v.attributes() for v in subG.vs[0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use iGraph to plot graphs, but it does not work great for larger graphs, and it is very hard to change the representation. This is why we turn to Gephi for actual visualizations, but I wanted you to get a sense of what this network looked like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t) plot the graph cell\n",
    "\n",
    "labels = ['@' + v['screen_name'] for v in subG.vs]\n",
    "ig.plot(subG, vertex_label=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about using libraries, is that we can calculate all kinds of network statistics from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u) calculate graph stats cell\n",
    "\n",
    "graph_stats = {}\n",
    "# Some very basic stats\n",
    "graph_stats['density'] = subG.density()\n",
    "graph_stats['num_nodes'] = subG.vcount()\n",
    "graph_stats['num_edges'] = subG.ecount()\n",
    "graph_stats['diameter'] = subG.diameter()\n",
    "\n",
    "# Stats about in and out degree. Note that these indegree()/outdegree() return a list\n",
    "# of the degree of each node. So we then take the average using a Numpy method \n",
    "# (Numpy is a math library very commonly used in Python)\n",
    "graph_stats['in_degree_mean'] = np.mean(subG.indegree())\n",
    "graph_stats['out_degree_mean'] = np.mean(subG.outdegree())\n",
    "graph_stats['degree_mean'] = np.mean(subG.degree())\n",
    "\n",
    "# Divide the graph into its components.\n",
    "# Specify we mean weakly connected, and then get each component as a subgraph\n",
    "components = subG.components(mode=ig.WEAK).subgraphs()\n",
    "\n",
    "# Sort these components in reverse order, using the number of nodes \n",
    "wccs = sorted(components, key=lambda g: g.vcount(), reverse=True)\n",
    "\n",
    "# then run some basic statistics on the largest component (which can be found at wccs[0])\n",
    "graph_stats['biggest_wcc_num_nodes'] = wccs[0].vcount()\n",
    "graph_stats['biggest_wcc_num_nodes_p'] = wccs[0].vcount()*100.0/subG.vcount()\n",
    "graph_stats['biggest_wcc_density'] = wccs[0].density()\n",
    "\n",
    "# Run the infomap algorithm on the largest component, and get its modularity score\n",
    "graph_stats['biggest_wcc_infomap_modularity'] = wccs[0].community_infomap().modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v) print out statscell\n",
    "\n",
    "for k,v in graph_stats.items():\n",
    "    if type(v) != int:\n",
    "        print('%s: %.2f' % (k,v))\n",
    "    else:\n",
    "        print('%s: %s' % (k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a basic, but complete example, of how you can take a list of tweets, collect all of the tweet, user, and follower information from Twitter, and then create a network in Python for analysis. \n",
    "\n",
    "This code would work on a much larger sample of tweets, but it may take a long time to complete. There are places where it could be made more efficient, and more robust (resistent to errors), but it was written like this for simplicity. \n",
    "\n",
    "When doing work like this with larger datasets, a few extra considerations need to be taken into account. These were beyond the scope of this course, but are listed here to give a sense of the scope of the work. \n",
    "\n",
    "We found that when working with older data, some tweets were no longer available (e.g. accounts may be private or deleted, and individual tweets may be deleted). The code needs to handle the empty returns, and you need to consider how to minimize the gaps in the data. For our diffusion network of scientific papers on Twitter, we tried to identify users by their screen_name when we could not fetch the tweet. This helped fill in the user information. However, for private accounts, we also needed a way to identify their follower information. In these cases, we used the follower networks of everyone else to fill the gaps (by getting the friends information, we get the other side of the relationship). \n",
    "\n",
    "There is also a lot of data management issues to work with. This tutorial showed how to write some files, but when working with multiple networks and thousands of tweets, more needs to be done. For example, we saved all of our tweet, user, and follower information in a local database (using SQLite). This helped us avoid fetching the same tweet or user information multiple times, even if the user appeared in our sample multiple times. Saving all the data locally as it is collected also avoids having to do all of the steps at once, since you can come back later and re-load some data without having to wait for the Twitter API. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
